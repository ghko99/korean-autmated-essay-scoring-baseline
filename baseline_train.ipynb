{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ef0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-22 18:22:12.703219: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-22 18:22:12.710683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753208532.719806   58981 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753208532.722507   58981 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753208532.729714   58981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753208532.729725   58981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753208532.729726   58981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753208532.729726   58981 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-22 18:22:12.732545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from baseline_embedding import get_dataset\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d9a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "\n",
    "args = config['nikl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49838058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b27a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_essay(essays, is_train=True):        \n",
    "    embedded_essay_raw = pd.read_csv(os.path.join(args['emb_file_path'], f\"{args['train_dataset_path'].split('/')[1]}_{'train' if is_train else 'valid'}_{'notlabeled' if args['is_topic_label'] == False else 'labeled'}.csv\"), encoding='cp949')\n",
    "    print(embedded_essay_raw.shape)\n",
    "    embedded_essay = []\n",
    "    tmp_ix = 0\n",
    "    for ix, essay_raw in enumerate(essays):\n",
    "        tmp_len = len(essay_raw)\n",
    "        essay = embedded_essay_raw[tmp_ix:tmp_ix + tmp_len]\n",
    "        embedded_essay.append(essay)\n",
    "        tmp_ix += tmp_len\n",
    "    return embedded_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd5decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_sent_pred, y_test):\n",
    "    metrics = {}\n",
    "    all_kappas = []\n",
    "    for i in range(len(args['rubric'])):\n",
    "        metrics[args['rubric'][i]] = {}\n",
    "        y_pred = y_sent_pred[:, i]\n",
    "        y_true = y_test[:, i]\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "        metrics[args['rubric'][i]]['accuracy'] = accuracy\n",
    "        metrics[args['rubric'][i]]['kappa'] = kappa\n",
    "        all_kappas.append(kappa)\n",
    "\n",
    "    metrics['mean'] = {}\n",
    "    overall_accuracy = accuracy_score(y_test.flatten(), y_sent_pred.flatten())\n",
    "    overall_kappa = np.mean(all_kappas)\n",
    "    metrics['mean']['accuracy'] = overall_accuracy\n",
    "    metrics['mean']['kappa'] = overall_kappa\n",
    "\n",
    "    metrics['overall'] = {}\n",
    "    metrics['overall']['kappa'] = cohen_kappa_score(y_test.flatten(), y_sent_pred.flatten(), weights='quadratic')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fbfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUScoreModule(nn.Module):\n",
    "    def __init__(self,output_dim,hidden_dim, dropout=0.5):\n",
    "        super(GRUScoreModule, self).__init__()\n",
    "        self.gru = nn.GRU(768,hidden_dim, dropout=dropout, batch_first=True, bidirectional=True)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        \n",
    "        x = x[:, -1, :]  # Use the output of the last time step\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fadf9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, embedded_essays, labels):\n",
    "        # 원본 데이터를 그대로 저장 (패딩하지 않음)\n",
    "        self.embedded_essays = embedded_essays\n",
    "        # labels만 텐서로 변환\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.maxlen = 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 필요할 때마다 패딩 및 텐서 변환 수행\n",
    "        essay = self.embedded_essays[idx]\n",
    "        \n",
    "        # numpy array로 변환 (DataFrame인 경우)\n",
    "        if hasattr(essay, 'values'):\n",
    "            essay = essay.values\n",
    "        \n",
    "        # 패딩 수행\n",
    "        if len(essay) < self.maxlen:\n",
    "            # pre-padding (앞쪽에 0 추가)\n",
    "            padded = np.zeros((self.maxlen, essay.shape[1]), dtype=np.float32)\n",
    "            padded[-len(essay):] = essay\n",
    "        else:\n",
    "            # maxlen까지만 자르기\n",
    "            padded = essay[:self.maxlen].astype(np.float32)\n",
    "        \n",
    "        # 텐서로 변환\n",
    "        padded_tensor = torch.tensor(padded, dtype=torch.float32)\n",
    "        \n",
    "        return padded_tensor, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8404be4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194416, 768)\n",
      "(22267, 768)\n"
     ]
    }
   ],
   "source": [
    "train_essay, valid_essay, train_y, valid_y = get_dataset()\n",
    "train_embedded_essay = get_embedded_essay(train_essay, is_train=True)\n",
    "valid_embedded_essay = get_embedded_essay(valid_essay, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ce4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EssayDataset(train_embedded_essay, train_y)\n",
    "valid_dataset = EssayDataset(valid_embedded_essay, valid_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f169f0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.5, 0.5, ..., 0.6, 0.8, 0.9],\n",
       "       [1. , 1. , 1. , ..., 0.8, 0.9, 0.9],\n",
       "       [0.9, 1. , 0.7, ..., 0.6, 0.7, 0.9],\n",
       "       ...,\n",
       "       [0.7, 1. , 0.9, ..., 0.8, 0.9, 0.9],\n",
       "       [0.8, 0.7, 0.7, ..., 0.7, 0.8, 0.9],\n",
       "       [0.4, 0.8, 0.5, ..., 0.4, 0.6, 0.8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ea91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b25fd970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "n_outputs = len(args['rubric'])\n",
    "\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "\n",
    "model = GRUScoreModule(output_dim=n_outputs,hidden_dim=128, dropout=dropout).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a474b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.0280, Val Loss: 0.0245, Time Elapsed: 2.2111\n",
      "Epoch 2/100, Train Loss: 0.0235, Val Loss: 0.0234, Time Elapsed: 2.0267\n",
      "Epoch 3/100, Train Loss: 0.0222, Val Loss: 0.0222, Time Elapsed: 2.2007\n",
      "Epoch 4/100, Train Loss: 0.0214, Val Loss: 0.0212, Time Elapsed: 2.3027\n",
      "Epoch 5/100, Train Loss: 0.0208, Val Loss: 0.0209, Time Elapsed: 2.4139\n",
      "Epoch 6/100, Train Loss: 0.0204, Val Loss: 0.0204, Time Elapsed: 2.5016\n",
      "Epoch 7/100, Train Loss: 0.0199, Val Loss: 0.0204, Time Elapsed: 2.5967\n",
      "Epoch 8/100, Train Loss: 0.0199, Val Loss: 0.0201, Time Elapsed: 2.6769\n",
      "Epoch 9/100, Train Loss: 0.0191, Val Loss: 0.0202, Time Elapsed: 2.7146\n",
      "Epoch 10/100, Train Loss: 0.0188, Val Loss: 0.0203, Time Elapsed: 2.7203\n",
      "Epoch 11/100, Train Loss: 0.0182, Val Loss: 0.0202, Time Elapsed: 2.6861\n",
      "Epoch 12/100, Train Loss: 0.0181, Val Loss: 0.0200, Time Elapsed: 2.7362\n",
      "Epoch 13/100, Train Loss: 0.0176, Val Loss: 0.0198, Time Elapsed: 2.7450\n",
      "Epoch 14/100, Train Loss: 0.0173, Val Loss: 0.0202, Time Elapsed: 2.7253\n",
      "Epoch 15/100, Train Loss: 0.0166, Val Loss: 0.0205, Time Elapsed: 2.7221\n",
      "Epoch 16/100, Train Loss: 0.0159, Val Loss: 0.0212, Time Elapsed: 2.7077\n",
      "Epoch 17/100, Train Loss: 0.0154, Val Loss: 0.0205, Time Elapsed: 2.6942\n",
      "Epoch 18/100, Train Loss: 0.0149, Val Loss: 0.0213, Time Elapsed: 2.7154\n",
      "Epoch 19/100, Train Loss: 0.0144, Val Loss: 0.0210, Time Elapsed: 2.7158\n",
      "Epoch 20/100, Train Loss: 0.0139, Val Loss: 0.0211, Time Elapsed: 2.6886\n",
      "Epoch 21/100, Train Loss: 0.0135, Val Loss: 0.0217, Time Elapsed: 2.7115\n",
      "Epoch 22/100, Train Loss: 0.0131, Val Loss: 0.0219, Time Elapsed: 2.7331\n",
      "Epoch 23/100, Train Loss: 0.0126, Val Loss: 0.0231, Time Elapsed: 2.8082\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "patience = 10\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "prev_time = time.time()\n",
    "set_seed(42)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs,labels in train_loader:\n",
    "        inputs ,labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)        \n",
    "        loss = criterion(outputs, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.cuda(),labels.cuda()\n",
    "            outputs = model(inputs)            \n",
    "            loss = criterion(outputs, labels)\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(valid_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time Elapsed: {time.time() - prev_time:.4f}')\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_outputs = np.array(all_outputs)\n",
    "        if not os.path.exists('./model'):\n",
    "            os.makedirs('./model')\n",
    "        torch.save(model.state_dict(), './model/kobert_model.pth')\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee96bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_outputs*5\n",
    "y_test = np.array(valid_y)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b4bc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(y_pred*2)/2\n",
    "pred = np.clip(pred, 1 , 5)\n",
    "pred = pred*2-1\n",
    "real = y_test*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dfe7e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'con1': {'accuracy': 0.26307053941908715, 'kappa': 0.4542025509728662},\n",
       " 'con2': {'accuracy': 0.29128630705394193, 'kappa': 0.413018728302731},\n",
       " 'con3': {'accuracy': 0.26639004149377593, 'kappa': 0.39591341644255507},\n",
       " 'con4': {'accuracy': 0.2846473029045643, 'kappa': 0.4612244318356269},\n",
       " 'con5': {'accuracy': 0.24066390041493776, 'kappa': 0.4237863476756063},\n",
       " 'org1': {'accuracy': 0.23402489626556017, 'kappa': 0.4440012063813844},\n",
       " 'org2': {'accuracy': 0.2854771784232365, 'kappa': 0.436483048240763},\n",
       " 'exp1': {'accuracy': 0.3269709543568465, 'kappa': 0.4291323519448814},\n",
       " 'exp2': {'accuracy': 0.30373443983402487, 'kappa': 0.502120365364841},\n",
       " 'mean': {'accuracy': 0.2773628400184417, 'kappa': 0.43998693857347276},\n",
       " 'overall': {'kappa': 0.5465917653248737}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = compute_metrics(pred,real)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d42cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
